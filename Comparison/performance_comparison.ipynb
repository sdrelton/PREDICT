{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from PREDICT import PREDICT\n",
    "from PREDICT.Models import *\n",
    "from PREDICT.Metrics import *\n",
    "from PREDICT.Triggers import *\n",
    "from PREDICT.Plots import *\n",
    "from Comparison.Detect_Functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Methods to Repair Temporal Drift\n",
    "\n",
    "In this notebook, four methods to repair temporal drift are compared:\n",
    "\n",
    "1) Regular model testing\n",
    "2) Statistical process control\n",
    "3) Static threshold\n",
    "4) Bayesian variable relative change\n",
    "\n",
    "\n",
    "\n",
    "These methods are compared for four scenarios:\n",
    "\n",
    "1) Fast predictor change - COVID pandemic\n",
    "2) Slow predictor change - population-based diabetes increase\n",
    "3) Outcome drift - change in prevalence of diabetes mellitus\n",
    "4) Multivariate drift - the diabetes prevalence increases whilst smoking prevalence decreases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = pd.to_datetime('01-06-2019', dayfirst=True) # 01-06-2019\n",
    "endDate = pd.to_datetime('31-12-2021', dayfirst=True) # 31-12-2021\n",
    "num_patients = 40 # number of patients per each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Change - COVID Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_runs = 0\n",
    "recalthreshold = 0.86 # Paper has AUROC of 0.91, with lower CI at 0.86\n",
    "\n",
    "custom_impacts = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 2.0, 2.5, 3.0] #0.1, 0.2, 0.3, 0.4, 0.5,0.6, 0.7, 0.8, 0.9,\n",
    "switchDateStrings = ['01-04-2020'] # Keep this as just one switchDate as other methods only look at one startDate/deployment date\n",
    "undetected = dict({\"Static Threshold\": 0, \"Regular Testing\": 0, \"SPC3\": 0, \"SPC5\":0, \"SPC7\":0, \"Bayesian\": 0})\n",
    "\n",
    "hr_age = 0.5\n",
    "hr_ldh = 9.8\n",
    "hr_comorbidity = 3.9\n",
    "\n",
    "log_age = np.log(hr_age)\n",
    "log_ldh = np.log(hr_ldh)\n",
    "log_comorbidity = np.log(hr_comorbidity)\n",
    "\n",
    "for switchDateidx, switchDateString in enumerate(switchDateStrings):\n",
    "    for custom_impact in custom_impacts:\n",
    "        mydict = {\n",
    "                'date': list(),\n",
    "                'outcome': list(),\n",
    "                'prediction': list(),\n",
    "                'age': list(),\n",
    "                'sex': list(),\n",
    "                'comorbidity': list(),\n",
    "                'ldh_high': list()\n",
    "            }\n",
    "\n",
    "        # Define date range and COVID shock periods\n",
    "        switchDate = pd.to_datetime(switchDateString, dayfirst=True)  # COVID starts spreading\n",
    "        switchDate2 = pd.to_datetime('01-06-2020', dayfirst=True)  # Peak of the pandemic\n",
    "        recoveryDate = pd.to_datetime('01-06-2021', dayfirst=True)  # Start of recovery phase\n",
    "        numdays = (endDate - startDate).days\n",
    "        switchDays = (switchDate - startDate).days\n",
    "        switch2Days = (switchDate2 - startDate).days\n",
    "        recoveryDays = (recoveryDate - startDate).days\n",
    "\n",
    "        for i in range(numdays):\n",
    "            curday = startDate + dt.timedelta(days=i)\n",
    "\n",
    "            age = (np.random.normal(44, 16.3, num_patients) - 44) / 16.3  # Mean age 44 years, std 16.3\n",
    "            sex = np.random.binomial(1, 0.562, num_patients) # 56.2% are male\n",
    "            comorbidity = np.random.binomial(1, 0.3, num_patients)  # 30% have comorbidities\n",
    "            ldh_high = np.random.binomial(1, 0.15, num_patients)  # 15% have LDH >500 U/L\n",
    "            epsilon = np.random.normal(0, 0.2, num_patients) # Simulate error term (mean=0, std=0.2)\n",
    "\n",
    "            # Calculate baseline log-odds\n",
    "            # sex influence 1.2 due to not being provided in the paper\n",
    "            lp = -1.5 + log_age * age +  log_ldh * ldh_high + log_comorbidity * comorbidity + 1.2 * (sex - 0.562) + epsilon\n",
    "            curpredictions = 1 / (1 + np.exp(-lp))  # Convert to probability\n",
    "\n",
    "            # Simulate COVID effects\n",
    "            if switchDays <= i < switch2Days:\n",
    "                lp += custom_impact  # Initial impact of COVID\n",
    "            elif switch2Days <= i < recoveryDays:\n",
    "                lp += custom_impact + 0.5  # Peak of the pandemic\n",
    "            elif i >= recoveryDays:\n",
    "                lp -= 1.0  # Recovery periodâ€”improved health outcomes\n",
    "\n",
    "            # Generate outcomes\n",
    "            curoutcomes = np.random.binomial(1, 1 / (1 + np.exp(-lp)))  # Simulate COVID events\n",
    "\n",
    "            # Append to dictionary\n",
    "            mydict['date'].extend([curday] * num_patients)\n",
    "            mydict['outcome'].extend(curoutcomes)\n",
    "            mydict['prediction'].extend(curpredictions)\n",
    "            mydict['age'].extend(age)\n",
    "            mydict['sex'].extend(sex)\n",
    "            mydict['comorbidity'].extend(comorbidity)\n",
    "            mydict['ldh_high'].extend(ldh_high)\n",
    "\n",
    "        df = pd.DataFrame(mydict)\n",
    "\n",
    "        covid_metrics_df = get_metrics_recal_methods(df, custom_impact, recalthreshold)\n",
    "\n",
    "        ########################################### Bayesian Testing #######################################\n",
    "        bay_model = BayesianModel(input_data=df, priors={\"Intercept\": (-1, 2), \"age\": (log_age, 2), \"sex\": (1, 2), \"comorbidity\": (log_comorbidity, 2), \"ldh_high\": (log_ldh, 2)}, cores=1, verbose=False, draws=1000, tune=250, chains=4)\n",
    "        bay_model.trigger = BayesianRefitTrigger(model=bay_model, input_data=df, refitFrequency=1)\n",
    "        mytest = PREDICT(data=df, model=bay_model, startDate='min', endDate='max', timestep='month')\n",
    "        mytest.addLogHook(Accuracy(bay_model))\n",
    "        mytest.addLogHook(AUROC(bay_model))\n",
    "        mytest.addLogHook(Precision(bay_model))\n",
    "        mytest.addLogHook(CalibrationSlope(bay_model))\n",
    "        mytest.addLogHook(CITL(bay_model))\n",
    "        mytest.addLogHook(OE(bay_model))\n",
    "        mytest.addLogHook(AUPRC(bay_model))\n",
    "        mytest.run()\n",
    "        log = mytest.getLog()\n",
    "\n",
    "        bayes_metrics = pd.DataFrame({'Time': list(log[\"Accuracy\"].keys()), 'Accuracy': list(log[\"Accuracy\"].values()), 'AUROC': list(log[\"AUROC\"].values()), 'Precision': list(log[\"Precision\"].values()), 'CalibrationSlope': list(log[\"CalibrationSlope\"].values()), 'CITL': list(log[\"CITL\"].values()), 'OE': list(log[\"O/E\"].values()), 'AUPRC': list(log[\"AUPRC\"].values()), 'impact_or_prev': list([str(custom_impact)] * len(log[\"Accuracy\"])), 'Method':list(['Bayesian'] * len(log[\"Accuracy\"]))})\n",
    "        \n",
    "        ########################################### Save Metrics #######################################\n",
    "\n",
    "        # concatenate all the dataframes into one\n",
    "        covid_metrics_df = pd.concat([covid_metrics_df, bayes_metrics], ignore_index=True)\n",
    "        covid_metrics_df[\"Data_Type\"] = \"COVID Simulation\"\n",
    "\n",
    "        covid_metrics_df.to_csv('C:/Users/scszha/CODERepository/PREDICT/Comparison/performance_metrics.csv', mode='a', header=False, index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome Prevalence - Diabetes Outcome Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_runs = 0\n",
    "recalthreshold = 0.77 # Paper has AUROC of 0.81, with lower CI at 0.77 \n",
    "\n",
    "prev_increases = np.arange(1.0001, 1.003, 0.0002).tolist() #[1.0001] \n",
    "undetected = dict({\"Static Threshold\": 0, \"Regular Testing\": 0, \"SPC3\": 0, \"SPC5\": 0, \"SPC7\": 0, \"Bayesian\": 0})\n",
    "\n",
    "# coefficients from non-laboratory logistic regression model\n",
    "age_at_lv_coef = 0.16 # lv = last visit\n",
    "bmi_coef = 0.68\n",
    "hip_circ_coef = -0.05\n",
    "sex_coef = -0.14\n",
    "height_coef = -0.15\n",
    "waist_circ_coef = 0.31\n",
    "waist_hips_ratio_coef = 0.54\n",
    "weight_coef = 0.03\n",
    "time_between_visits_coef = 0.38\n",
    "bias_coef = -0.74\n",
    "\n",
    "# mean and standard deviation for each predictor\n",
    "# variable at the last visit is used\n",
    "mean_age, std_age = 62.9, 7.5\n",
    "mean_bmi, std_bmi = 26.6, 4.4\n",
    "mean_hip_circ, std_hip_circ = 101.6, 8.8\n",
    "perc_male, mean_height, std_height = 0.478, 169, 9.2\n",
    "mean_waist_circ, std_waist_circ = 88.7, 12.7\n",
    "mean_weight, std_weight = 76.2, 15.2\n",
    "mean_time_between_visits, std_time_between_visits = 7.3, 2.3\n",
    "\n",
    "mean_waist_hips_ratio = mean_waist_circ / mean_hip_circ\n",
    "std_waist_hips_ratio = mean_waist_hips_ratio * np.sqrt(\n",
    "    (std_waist_circ / mean_waist_circ) ** 2 + (std_hip_circ / mean_hip_circ) ** 2)\n",
    "\n",
    "dm_prev = 0.07  # Initial diabetes prevalence = 7%\n",
    "for prev_increase in prev_increases:\n",
    "    mydict = {\n",
    "            'date': list(),\n",
    "            'outcome': list(),\n",
    "            'prediction': list(),\n",
    "            'age': list(),\n",
    "            'bmi':list(),\n",
    "            'hip_circ': list(),\n",
    "            'sex': list(),\n",
    "            'height': list(),\n",
    "            'waist_circ': list(),\n",
    "            'waist_hips_ratio': list(),\n",
    "            'weight': list(),\n",
    "            'time_between_visits': list()\n",
    "        }\n",
    "\n",
    "    num_patients = 60\n",
    "    numdays = (endDate - startDate).days\n",
    "    \n",
    "    for i in range(numdays):\n",
    "        curday = startDate + dt.timedelta(days=i)\n",
    "\n",
    "        age = np.random.normal(mean_age, std_age, num_patients)\n",
    "        # min max normalisation\n",
    "        age = (age - np.min(age)) / (np.max(age) - np.min(age))  # Normalize age to [0, 1]\n",
    "\n",
    "        bmi = np.random.normal(mean_bmi, std_bmi, num_patients) \n",
    "        bmi = (bmi - np.min(bmi)) / (np.max(bmi) - np.min(bmi))  # Normalize BMI to [0, 1]\n",
    "\n",
    "        hip_circ = np.random.normal(mean_hip_circ, std_hip_circ, num_patients)\n",
    "        hip_circ = (hip_circ - np.min(hip_circ)) / (np.max(hip_circ) - np.min(hip_circ))\n",
    "\n",
    "        height = np.random.normal(mean_height, std_height, num_patients)\n",
    "        height = (height - np.min(height)) / (np.max(height) - np.min(height))  # Normalize height to [0, 1]\n",
    "\n",
    "        waist_circ = np.random.normal(mean_waist_circ, std_waist_circ, num_patients)\n",
    "        waist_circ = (waist_circ - np.min(waist_circ)) / (np.max(waist_circ) - np.min(waist_circ))  # Normalize waist circumference to [0, 1]\n",
    "\n",
    "        waist_hips_ratio = np.random.normal(mean_waist_hips_ratio, std_waist_hips_ratio, num_patients)\n",
    "        waist_hips_ratio = (waist_hips_ratio - np.min(waist_hips_ratio)) / (np.max(waist_hips_ratio) - np.min(waist_hips_ratio))  # Normalize waist-hips ratio to [0, 1]\n",
    "\n",
    "        weight = np.random.normal(mean_weight, std_weight, num_patients)\n",
    "        weight = (weight - np.min(weight)) / (np.max(weight) - np.min(weight))  # Normalize weight to [0, 1]\n",
    "\n",
    "        time_between_visits = np.random.normal(mean_time_between_visits, std_time_between_visits, num_patients)\n",
    "        time_between_visits = (time_between_visits - np.min(time_between_visits)) / (np.max(time_between_visits) - np.min(time_between_visits))  # Normalize time between visits to [0, 1]\n",
    "\n",
    "        sex = np.random.binomial(1, perc_male, num_patients)\n",
    "\n",
    "        epsilon = np.random.normal(0, 0.2, num_patients) # Simulate error term (mean=0, std=0.2)\n",
    "        \n",
    "\n",
    "        # Calculate baseline log-odds\n",
    "        lp = bias_coef + age_at_lv_coef * age + bmi_coef * bmi + hip_circ_coef * hip_circ + sex_coef * (sex - perc_male) + height_coef * height + waist_circ_coef * waist_circ  + waist_hips_ratio_coef * waist_hips_ratio + weight_coef * weight  + time_between_visits_coef * time_between_visits + epsilon\n",
    "        \n",
    "        curpredictions = 1 / (1 + np.exp(-lp))  # Convert to probability\n",
    "\n",
    "        # Generate outcomes to simulate diabetes rates increasing over time\n",
    "        if i % 30 == 0:\n",
    "            dm_prev *= prev_increase # this increases the probability by x% each month\n",
    "\n",
    "        mod_lp = 1/(1+np.exp(lp + dm_prev))\n",
    "        # intercept changed, but model weights constant\n",
    "        # diabetes increased as outcome, but not explained by data\n",
    "        curoutcomes = np.random.binomial(1, mod_lp)           \n",
    "        \n",
    "\n",
    "        # Append to dictionary from the distribution for each of the variables (Table 1)\n",
    "        mydict['date'].extend([curday] * num_patients)\n",
    "        mydict['outcome'].extend(curoutcomes)\n",
    "        mydict['prediction'].extend(curpredictions)\n",
    "        mydict['age'].extend(age)\n",
    "        mydict['bmi'].extend(bmi)\n",
    "        mydict['hip_circ'].extend(hip_circ)\n",
    "        mydict['sex'].extend(sex)\n",
    "        mydict['height'].extend(height)\n",
    "        mydict['waist_circ'].extend(waist_circ)\n",
    "        mydict['waist_hips_ratio'].extend(waist_hips_ratio)\n",
    "        mydict['weight'].extend(weight)\n",
    "        mydict['time_between_visits'].extend(time_between_visits)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(mydict)  \n",
    "    out_prev_metrics_df = get_metrics_recal_methods(df, dm_prev, recalthreshold)\n",
    "    \n",
    "    \n",
    "    ########################################### Bayesian Testing #######################################\n",
    "    bay_model = BayesianModel(input_data=df, priors={\"Intercept\": (bias_coef, 2), \"age\": (age_at_lv_coef, 2), \"bmi\": (bmi_coef, 2), \"hip_circ\": (hip_circ_coef, 2),\n",
    "                                                \"sex\": (sex_coef, 2), \"height\":(height_coef, 2), \"waist_circ\":(waist_circ_coef, 2),\n",
    "                                                \"waist_hips_ratio\":(waist_hips_ratio_coef, 2), \"weight\":(weight_coef, 2), \n",
    "                                                \"time_between_visits\":(time_between_visits_coef, 2)}, cores=1, verbose=False)\n",
    "    bay_model.trigger = BayesianRefitTrigger(model=bay_model, input_data=df, refitFrequency=1)\n",
    "    mytest = PREDICT(data=df, model=bay_model, startDate='min', endDate='max', timestep='month')\n",
    "    mytest.addLogHook(Accuracy(bay_model))\n",
    "    mytest.addLogHook(AUROC(bay_model))\n",
    "    mytest.addLogHook(Precision(bay_model))\n",
    "    mytest.addLogHook(CalibrationSlope(bay_model))\n",
    "    mytest.addLogHook(CITL(bay_model))\n",
    "    mytest.addLogHook(OE(bay_model))\n",
    "    mytest.addLogHook(AUPRC(bay_model))\n",
    "    mytest.run()\n",
    "    log = mytest.getLog()\n",
    "\n",
    "    bayes_metrics = pd.DataFrame({'Time': list(log[\"Accuracy\"].keys()), 'Accuracy': list(log[\"Accuracy\"].values()), 'AUROC': list(log[\"AUROC\"].values()), 'Precision': list(log[\"Precision\"].values()), 'CalibrationSlope': list(log[\"CalibrationSlope\"].values()), 'CITL': list(log[\"CITL\"].values()), 'OE': list(log[\"O/E\"].values()), 'AUPRC': list(log[\"AUPRC\"].values()), 'impact_or_prev': list([str(dm_prev)] * len(log[\"Accuracy\"])), 'Method':list(['Bayesian'] * len(log[\"Accuracy\"]))})\n",
    "    \n",
    "    ########################################### Save Metrics #######################################\n",
    "\n",
    "    # concatenate all the dataframes into one\n",
    "    out_prev_metrics_df = pd.concat([out_prev_metrics_df, bayes_metrics], ignore_index=True)\n",
    "    out_prev_metrics_df[\"Data_Type\"] = \"Outcome Prevalence Simulation\"\n",
    "\n",
    "    out_prev_metrics_df.to_csv('C:/Users/scszha/CODERepository/PREDICT/Comparison/performance_metrics.csv', mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slow change data simulation - Diabetes as a Predictor (increasing over time) with CKD as the predicted outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_runs = 0\n",
    "recalthreshold = 0.851 # Paper has AUROC of 0.889, with lower CI at 0.851\n",
    "\n",
    "prev_increases = np.arange(1.0001, 1.0030, 0.0002).tolist() #[1.0001] \n",
    "undetected = dict({\"Static Threshold\": 0, \"Regular Testing\": 0, \"SPC3\": 0, \"SPC5\": 0, \"SPC7\": 0, \"Bayesian\": 0})\n",
    "\n",
    "mean_TGFB, std_TGFB = 13.23, 5.18\n",
    "mean_ADMA, std_ADMA= 101.1, 64.8\n",
    "mean_BUN, std_BUN = 5.45, 1.11\n",
    "mean_age, std_age = 63.27, 10.09 \n",
    "\n",
    "TGFB_coef = 1.84\n",
    "ADMA_coef = 1.137\n",
    "DM_coef = 0.84\n",
    "BUN_coef = 0.497\n",
    "elderly_coef = 0.603\n",
    "\n",
    "perc_dm = 0.05 # 5.5%\n",
    "for prev_increase in prev_increases:\n",
    "    mydict = {\n",
    "            'date': list(),\n",
    "            'outcome': list(),\n",
    "            'prediction': list(),\n",
    "            'TGFB': list(),\n",
    "            'ADMA':list(),\n",
    "            'DM': list(),\n",
    "            'BUN': list(),\n",
    "            'elderly': list()\n",
    "        }\n",
    "\n",
    "    num_patients = 60\n",
    "\n",
    "    numdays = (endDate - startDate).days\n",
    "\n",
    "    for i in range(numdays):\n",
    "        curday = startDate + dt.timedelta(days=i)\n",
    "\n",
    "        # increase the prevalence of diabetes over time\n",
    "        if i % 30 == 0:\n",
    "            perc_dm *= prev_increase # this increases the probability by x% each month\n",
    "\n",
    "        TGFB = get_binom_from_normal(mean_TGFB, std_TGFB, num_patients, 1.011)\n",
    "        ADMA = get_binom_from_normal(mean_ADMA, std_ADMA, num_patients, 0.019)\n",
    "        DM = np.random.binomial(1, perc_dm, num_patients)\n",
    "        BUN = get_binom_from_normal(mean_BUN, std_BUN, num_patients, 5.9)\n",
    "        elderly = get_binom_from_normal(mean_age, std_age, num_patients, 60)\n",
    "        epsilon = np.random.normal(0, 0.2, num_patients) # Simulate error term (mean=0, std=0.2)\n",
    "\n",
    "        # Calculate baseline log-odds\n",
    "        # non_genetic_risk_score_model from paper\n",
    "        lp = TGFB_coef * TGFB + ADMA_coef * ADMA + DM_coef * DM + BUN_coef * BUN + elderly_coef * elderly + epsilon\n",
    "\n",
    "        curpredictions = 1 / (1 + np.exp(-lp))  # Convert to probability\n",
    "        curoutcomes = np.random.binomial(1, curpredictions)           \n",
    "        \n",
    "        # Append to dictionary from the distribution for each of the variables (Table 1)\n",
    "        mydict['date'].extend([curday] * num_patients)\n",
    "        mydict['outcome'].extend(curoutcomes)\n",
    "        mydict['prediction'].extend(curpredictions)\n",
    "        mydict['TGFB'].extend(TGFB)\n",
    "        mydict['ADMA'].extend(ADMA)\n",
    "        mydict['DM'].extend(DM)\n",
    "        mydict['BUN'].extend(BUN)\n",
    "        mydict['elderly'].extend(elderly)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(mydict)\n",
    "    slow_change_metrics_df = get_metrics_recal_methods(df, perc_dm, recalthreshold)\n",
    "    \n",
    "\n",
    "    ########################################### Bayesian Testing #######################################\n",
    "    bay_model = BayesianModel(input_data=df, priors={\"Intercept\": (-1, 2), \"TGFB\": (TGFB_coef, 2), \"ADMA\": (ADMA_coef, 2), \"DM\": (DM_coef, 2), \"BUN\": (BUN_coef, 2),\n",
    "                                                \"elderly\": (elderly_coef, 2)}, cores=1, verbose=False)\n",
    "    bay_model.trigger = BayesianRefitTrigger(model=bay_model, input_data=df, refitFrequency=1)\n",
    "    mytest = PREDICT(data=df, model=bay_model, startDate='min', endDate='max', timestep='month')\n",
    "    mytest.addLogHook(Accuracy(bay_model))\n",
    "    mytest.addLogHook(AUROC(bay_model))\n",
    "    mytest.addLogHook(Precision(bay_model))\n",
    "    mytest.addLogHook(CalibrationSlope(bay_model))\n",
    "    mytest.addLogHook(CITL(bay_model))\n",
    "    mytest.addLogHook(OE(bay_model))\n",
    "    mytest.addLogHook(AUPRC(bay_model))\n",
    "    mytest.run()\n",
    "    log = mytest.getLog()\n",
    "\n",
    "    bayes_metrics = pd.DataFrame({'Time': list(log[\"Accuracy\"].keys()), 'Accuracy': list(log[\"Accuracy\"].values()), 'AUROC': list(log[\"AUROC\"].values()), 'Precision': list(log[\"Precision\"].values()), 'CalibrationSlope': list(log[\"CalibrationSlope\"].values()), 'CITL': list(log[\"CITL\"].values()), 'OE': list(log[\"O/E\"].values()), 'AUPRC': list(log[\"AUPRC\"].values()), 'impact_or_prev': list([str(perc_dm)] * len(log[\"Accuracy\"])), 'Method':list(['Bayesian'] * len(log[\"Accuracy\"]))})\n",
    "    \n",
    "    ########################################### Save Metrics #######################################\n",
    "\n",
    "    # concatenate all the dataframes into one\n",
    "    slow_change_metrics_df = pd.concat([slow_change_metrics_df, bayes_metrics], ignore_index=True)\n",
    "    slow_change_metrics_df[\"Data_Type\"] = \"Slow Change Simulation\"\n",
    "\n",
    "    slow_change_metrics_df.to_csv('performance_metrics.csv', mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Model - QRISK2 - Diabetes increasing whilst smoking is decreasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalthreshold = 0.811 # Paper has AUROC of 0.814, with lower CI at 0.811 \n",
    "\n",
    "# Define the coefficients (hazard ratios converted to log-odds)\n",
    "coefs = {\"White\": np.log(1), \n",
    "    \"Indian\": np.log(1.43),\n",
    "    \"Pakistani\": np.log(1.8),\n",
    "    \"Bangladeshi\": np.log(1.35),\n",
    "    \"Other_Asian\": np.log(1.15),\n",
    "    \"Black_Caribbean\": np.log(1.08),\n",
    "    \"Black_African\": np.log(0.58),\n",
    "    \"Chinese\": np.log(0.69),\n",
    "    \"Other\": np.log(1.04),\n",
    "    \"Age\": np.log(1.66),\n",
    "    \"BMI\": np.log(1.08),\n",
    "    \"Townsend\": np.log(1.37),\n",
    "    \"SBP\": np.log(1.2),\n",
    "    \"CholHDL_ratio\": np.log(1.17),\n",
    "    \"Family_CHD\": np.log(1.99),\n",
    "    \"Current_smoker\": np.log(1.8),\n",
    "    \"Treated_HTN\": np.log(1.54),\n",
    "    \"DM\": np.log(2.54),\n",
    "    \"RA\": np.log(1.5),\n",
    "    \"AF\": np.log(3.06),\n",
    "    \"Renal_disease\": np.log(1.7),\n",
    "    \"Age_BMI\": np.log(0.976),\n",
    "    \"Age_Townsend\": np.log(0.938),\n",
    "    \"Age_SBP\": np.log(0.966),\n",
    "    \"Age_Family_CHD\": np.log(0.927),\n",
    "    \"Age_Smoking\": np.log(0.931),\n",
    "    \"Age_Treated_HTN\": np.log(0.952),\n",
    "    \"Age_DM\": np.log(0.904),\n",
    "    \"Age_AF\": np.log(0.858)\n",
    "}\n",
    "\n",
    "\n",
    "# Percentage variables (/100)\n",
    "percent_family_history_chd = 0.126\n",
    "percent_treated_hypertension = 0.0712\n",
    "percent_rheumatoid_arthritis = 0.0093\n",
    "percent_atrial_fibrillation = 0.0035\n",
    "percent_renal_disease = 0.0016\n",
    "\n",
    "# Age variable\n",
    "median_age, IQR_age = 49, 19\n",
    "mean_age, std_age = median_age, IQR_age / 1.35\n",
    "\n",
    "# Mean and standard deviation variables\n",
    "mean_bmi, std_bmi = 33.8, 6.1\n",
    "mean_townsend, std_townsend = 17.67, 3.534\n",
    "mean_sbp, std_sbp = 26.6, 4.4\n",
    "mean_chol_hdl_ratio, std_chol_hdl_ratio = 3.66, 0.144\n",
    "\n",
    "\n",
    "intercept = None\n",
    "baseline_prob = 0.233 # 23.3%\n",
    "total_runs = 0\n",
    "\n",
    "prev_increases = np.arange(1.0001, 1.003, 0.0002).tolist() # Increase in diabetes prevalence over time \n",
    "smoking_decrease = np.arange(0.9995, 0.9967, -0.0002).tolist()  # Decrease in smoking prevalence over time\n",
    "undetected = dict({\"Static Threshold\": 0, \"Regular Testing\": 0, \"SPC3\": 0, \"SPC5\": 0, \"SPC7\": 0, \"Bayesian\": 0})\n",
    "\n",
    "percent_type_2_diabetes = 0.017 # reset these for each start date\n",
    "percent_current_smoker = 0.228\n",
    "for num, prev_increase in enumerate(prev_increases):\n",
    "    mydict = {\n",
    "            'date': list(),\n",
    "            'outcome': list(),\n",
    "            'prediction': list(),\n",
    "            'White': list(),\n",
    "            'Indian': list(),\n",
    "            'Pakistani': list(),\n",
    "            'Bangladeshi': list(),\n",
    "            'Other_Asian': list(),\n",
    "            'Black_Caribbean': list(),\n",
    "            'Black_African': list(),\n",
    "            'Chinese': list(),\n",
    "            'Other': list(),\n",
    "            'Age': list(),\n",
    "            'BMI':list(),\n",
    "            'Townsend': list(),\n",
    "            'SBP': list(),\n",
    "            'CholHDL_ratio': list(),\n",
    "            'Family_CHD': list(),\n",
    "            'Current_smoker': list(),\n",
    "            'Treated_HTN': list(),\n",
    "            'DM': list(),\n",
    "            'RA': list(),\n",
    "            'AF': list(),\n",
    "            'Renal_disease': list()\n",
    "        }\n",
    "\n",
    "\n",
    "    # Define date range\n",
    "    numdays = (endDate - startDate).days\n",
    "\n",
    "    \n",
    "    for i in range(numdays):\n",
    "        curday = startDate + dt.timedelta(days=i)\n",
    "\n",
    "        # increase the prevalence of diabetes over time\n",
    "        if i % 30 == 0:\n",
    "            percent_type_2_diabetes *= prev_increase # this increases the probability by x% each month\n",
    "            percent_current_smoker *= smoking_decrease[num] # decrease the prevalence of smoking over time\n",
    "        if percent_type_2_diabetes < 0 or percent_type_2_diabetes > 1:\n",
    "            print(\"Percentage of people with DM\", percent_type_2_diabetes)\n",
    "        if percent_current_smoker < 0 or percent_current_smoker > 1:\n",
    "            print(\"Percentage of people who are current smokers\", percent_current_smoker)\n",
    "\n",
    "        # Generate random factors for patients using min max normalization for non-binary values\n",
    "        age = np.random.normal(mean_age, std_age, num_patients) \n",
    "        age = (age - np.min(age)) / (np.max(age) - np.min(age))\n",
    "        bmi = np.random.normal(mean_bmi, std_bmi, num_patients)\n",
    "        bmi = (bmi - np.min(bmi)) / (np.max(bmi) - np.min(bmi))\n",
    "        townsend = np.random.normal(mean_townsend, std_townsend, num_patients)\n",
    "        townsend = (townsend - np.min(townsend)) / (np.max(townsend) - np.min(townsend))\n",
    "        SBP = np.random.normal(mean_sbp, std_sbp, num_patients)\n",
    "        SBP = (SBP - np.min(SBP)) / (np.max(SBP) - np.min(SBP))\n",
    "        chol_hdl_ratio = np.random.normal(mean_chol_hdl_ratio, std_chol_hdl_ratio, num_patients)\n",
    "        chol_hdl_ratio = (chol_hdl_ratio - np.min(chol_hdl_ratio)) / (np.max(chol_hdl_ratio) - np.min(chol_hdl_ratio))\n",
    "        pat_factors = {\"Age\": age, \n",
    "            \"BMI\": bmi,\n",
    "            \"Townsend\": townsend,\n",
    "            \"SBP\": SBP,\n",
    "            \"CholHDL_ratio\": chol_hdl_ratio,\n",
    "            \"Family_CHD\": np.random.binomial(1, percent_family_history_chd, num_patients),\n",
    "            \"Current_smoker\": np.random.binomial(1, percent_current_smoker, num_patients),\n",
    "            \"Treated_HTN\": np.random.binomial(1, percent_treated_hypertension, num_patients),\n",
    "            \"DM\": np.random.binomial(1, percent_type_2_diabetes, num_patients),\n",
    "            \"RA\": np.random.binomial(1, percent_rheumatoid_arthritis, num_patients),\n",
    "            \"AF\": np.random.binomial(1, percent_atrial_fibrillation, num_patients),\n",
    "            \"Renal_disease\": np.random.binomial(1, percent_renal_disease, num_patients)\n",
    "        }\n",
    "        epsilon = np.random.normal(0, 0.2, num_patients) # Simulate error term (mean=0, std=0.2)\n",
    "\n",
    "        ethnicity_assignment = select_ethnic_group(num_patients)\n",
    "        pat_factors.update(ethnicity_assignment) # combine ethnicity dict with ethnic\n",
    "\n",
    "        # Calculate baseline log-odds\n",
    "        weighted_coef_sum = coefs['White']*pat_factors['White'] + coefs['Indian']*pat_factors['Indian'] + coefs['Pakistani']*pat_factors['Pakistani'] + coefs['Bangladeshi']*pat_factors['Bangladeshi'] \n",
    "        weighted_coef_sum += coefs['Other_Asian']*pat_factors['Other_Asian'] + coefs['Black_Caribbean']*pat_factors['Black_Caribbean'] + coefs['Black_African']*pat_factors['Black_African'] \n",
    "        weighted_coef_sum += coefs['Chinese']*pat_factors['Chinese'] + coefs['Other']*pat_factors['Other'] + coefs['Age']*(pat_factors['Age']) + coefs['BMI']*(pat_factors['BMI']) \n",
    "        weighted_coef_sum += coefs['Townsend']*(pat_factors['Townsend']) + coefs['SBP']*(pat_factors['SBP']) + coefs['CholHDL_ratio']*(pat_factors['CholHDL_ratio']) \n",
    "        weighted_coef_sum += coefs[\"Family_CHD\"]*(pat_factors[\"Family_CHD\"]) + coefs[\"Current_smoker\"]*(pat_factors[\"Current_smoker\"]) \n",
    "        weighted_coef_sum += coefs[\"Treated_HTN\"]*(pat_factors[\"Treated_HTN\"]) + coefs[\"DM\"]*(pat_factors[\"DM\"]) + coefs[\"RA\"]*(pat_factors[\"RA\"]) \n",
    "        weighted_coef_sum += coefs[\"AF\"]*(pat_factors[\"AF\"]) + coefs[\"Renal_disease\"]*(pat_factors[\"Renal_disease\"]) + (coefs[\"Age_BMI\"] * pat_factors[\"Age\"] * pat_factors[\"BMI\"]) \n",
    "        weighted_coef_sum += (coefs[\"Age_Townsend\"] * pat_factors[\"Age\"] * pat_factors[\"Townsend\"]) + (coefs[\"Age_SBP\"] * pat_factors[\"Age\"] * pat_factors[\"SBP\"]) \n",
    "        weighted_coef_sum += (coefs[\"Age_Family_CHD\"] * pat_factors[\"Age\"] * pat_factors[\"Family_CHD\"]) + (coefs[\"Age_Smoking\"] * pat_factors[\"Age\"] * pat_factors[\"Current_smoker\"]) \n",
    "        weighted_coef_sum += (coefs[\"Age_Treated_HTN\"] * pat_factors[\"Age\"] * pat_factors[\"Treated_HTN\"]) + (coefs[\"Age_DM\"] * pat_factors[\"Age\"] * pat_factors[\"DM\"])\n",
    "        weighted_coef_sum += (coefs[\"Age_AF\"] * pat_factors[\"Age\"] * pat_factors[\"AF\"]) + epsilon\n",
    "\n",
    "    \n",
    "        intercept = np.log(baseline_prob / (1 - baseline_prob))\n",
    "        \n",
    "        # Compute log-odds\n",
    "        lp = intercept + weighted_coef_sum\n",
    "        lp = np.clip(lp, -500, 500)  # Clip to avoid overflow issues\n",
    "        \n",
    "        curpredictions = 1 / (1 + np.exp(-lp))  # Convert to probability\n",
    "        \n",
    "        \n",
    "        curoutcomes = np.random.binomial(1, curpredictions)         \n",
    "        \n",
    "\n",
    "        # Append to dictionary from the distribution for each of the variables (Table 1)\n",
    "        mydict['date'].extend([curday] * num_patients)\n",
    "        mydict['outcome'].extend(curoutcomes)\n",
    "        mydict['prediction'].extend(curpredictions)\n",
    "        mydict['White'].extend(pat_factors['White'])\n",
    "        mydict['Indian'].extend(pat_factors['Indian'])\n",
    "        mydict['Pakistani'].extend(pat_factors['Pakistani'])\n",
    "        mydict['Bangladeshi'].extend(pat_factors['Bangladeshi'])\n",
    "        mydict['Other_Asian'].extend(pat_factors['Other_Asian'])\n",
    "        mydict['Black_Caribbean'].extend(pat_factors['Black_Caribbean'])\n",
    "        mydict['Black_African'].extend(pat_factors['Black_African'])\n",
    "        mydict['Chinese'].extend(pat_factors['Chinese'])\n",
    "        mydict['Other'].extend(pat_factors['Other'])\n",
    "        mydict['Age'].extend(pat_factors['Age'])\n",
    "        mydict['BMI'].extend(pat_factors['BMI'])\n",
    "        mydict['Townsend'].extend(pat_factors['Townsend'])\n",
    "        mydict['SBP'].extend(pat_factors['SBP'])\n",
    "        mydict['CholHDL_ratio'].extend(pat_factors['CholHDL_ratio'])\n",
    "        mydict['Family_CHD'].extend(pat_factors['Family_CHD'])\n",
    "        mydict['Current_smoker'].extend(pat_factors['Current_smoker'])\n",
    "        mydict['Treated_HTN'].extend(pat_factors['Treated_HTN'])\n",
    "        mydict['DM'].extend(pat_factors['DM'])\n",
    "        mydict['RA'].extend(pat_factors['RA'])\n",
    "        mydict['AF'].extend(pat_factors['AF'])\n",
    "        mydict['Renal_disease'].extend(pat_factors['Renal_disease'])\n",
    "\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(mydict)\n",
    "            \n",
    "    df = prevent_constant_variable(df, startDate, endDate)\n",
    "\n",
    "    multivariate_metrics_df = get_metrics_recal_methods(df, percent_type_2_diabetes, recalthreshold)\n",
    "    \n",
    "    \n",
    "    ########################################### Bayesian Testing #######################################\n",
    "    bay_model = BayesianModel(input_data=df, priors={\"Intercept\": (intercept, 2),\n",
    "                                                \"White\": (coefs['White'], 2), \n",
    "                                                \"Indian\": (coefs['Indian'], 2),\n",
    "                                                \"Pakistani\": (coefs['Pakistani'], 2),\n",
    "                                                \"Bangladeshi\": (coefs['Bangladeshi'], 2),\n",
    "                                                \"Other_Asian\": (coefs['Other_Asian'], 2),\n",
    "                                                \"Black_Caribbean\": (coefs['Black_Caribbean'], 2),\n",
    "                                                \"Black_African\": (coefs['Black_African'], 2),\n",
    "                                                \"Chinese\": (coefs['Chinese'], 2),\n",
    "                                                \"Other\": (coefs['Other'], 2),\n",
    "                                                \"Age\": (coefs['Age'], 2),\n",
    "                                                \"BMI\": (coefs['BMI'], 2),\n",
    "                                                \"Townsend\": (coefs['Townsend'], 2),\n",
    "                                                \"SBP\": (coefs['SBP'], 2),\n",
    "                                                \"CholHDL_ratio\": (coefs['CholHDL_ratio'], 2),\n",
    "                                                \"Family_CHD\": (coefs['Family_CHD'], 2),\n",
    "                                                \"Current_smoker\": (coefs['Current_smoker'], 2),\n",
    "                                                \"Treated_HTN\": (coefs['Treated_HTN'], 2),\n",
    "                                                \"DM\": (coefs['DM'], 2),\n",
    "                                                \"RA\": (coefs['RA'], 2),\n",
    "                                                \"AF\": (coefs['AF'], 2),\n",
    "                                                \"Renal_disease\": (coefs['Renal_disease'], 2)}, \n",
    "                                                cores=1, verbose=False,\n",
    "                                                model_formula=\"outcome ~ White + Indian + Pakistani + Bangladeshi + Other_Asian + Black_Caribbean + Black_African + Chinese + Other + Age + BMI + Townsend + SBP + CholHDL_ratio + Family_CHD + Current_smoker + Treated_HTN + DM + RA + AF + Renal_disease + Age*BMI + Age*Townsend + Age*SBP + Age*Family_CHD + Age*Current_smoker + Age*Treated_HTN + Age*DM + Age*AF\")\n",
    "    bay_model.trigger = BayesianRefitTrigger(model=bay_model, input_data=df, refitFrequency=1)\n",
    "    mytest = PREDICT(data=df, model=bay_model, startDate='min', endDate='max', timestep='month')\n",
    "    mytest.addLogHook(Accuracy(bay_model))\n",
    "    mytest.addLogHook(AUROC(bay_model))\n",
    "    mytest.addLogHook(Precision(bay_model))\n",
    "    mytest.addLogHook(CalibrationSlope(bay_model))\n",
    "    mytest.addLogHook(CITL(bay_model))\n",
    "    mytest.addLogHook(OE(bay_model))\n",
    "    mytest.addLogHook(AUPRC(bay_model))\n",
    "    mytest.run()\n",
    "    log = mytest.getLog()\n",
    "\n",
    "    bayes_metrics = pd.DataFrame({'Time': list(log[\"Accuracy\"].keys()), 'Accuracy': list(log[\"Accuracy\"].values()), 'AUROC': list(log[\"AUROC\"].values()), 'Precision': list(log[\"Precision\"].values()), 'CalibrationSlope': list(log[\"CalibrationSlope\"].values()), 'CITL': list(log[\"CITL\"].values()), 'OE': list(log[\"O/E\"].values()), 'AUPRC': list(log[\"AUPRC\"].values()), 'impact_or_prev': list([str(percent_type_2_diabetes)] * len(log[\"Accuracy\"])), 'Method':list(['Bayesian'] * len(log[\"Accuracy\"]))})\n",
    "    \n",
    "    ########################################### Save Metrics #######################################\n",
    "\n",
    "    # concatenate all the dataframes into one\n",
    "    multivariate_metrics_df = pd.concat([multivariate_metrics_df, bayes_metrics], ignore_index=True)\n",
    "    multivariate_metrics_df[\"Data_Type\"] = \"Multivariate Simulation\"\n",
    "    \n",
    "    multivariate_metrics_df.to_csv('performance_metrics.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results from metric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "metrics_df = pd.read_csv('performance_metrics.csv')\n",
    "metrics_df[\"Time\"] = pd.to_datetime(metrics_df[\"Time\"])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the average metric each month for each impact score or prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# # Loop through each unique Data_type to create individual plots\n",
    "# for data_type, subset in metrics_df.groupby(\"Data_type\"):\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "    \n",
    "#     # Plot each unique combination of impact_or_prev and method within this Data_type\n",
    "#     for group, subsubset in subset.groupby([\"impact_or_prev\", \"Method\"]):\n",
    "#         label = f\"{group[0]} - {group[1]}\"  # Combining impact and method for legend\n",
    "#         plt.plot(subsubset[\"Time\"], subsubset[\"Accuracy\"], label=label)\n",
    "\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.title(f\"Accuracy Trends for {data_type}\")\n",
    "#     plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the average metric each month for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Time, Method, and Data_type and get the average of the metric\n",
    "numeric_cols = [\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\"]\n",
    "df_avg = metrics_df.groupby([\"Time\", \"Method\", \"Data_type\"], as_index=False)[numeric_cols].mean()\n",
    "\n",
    "\n",
    "metric_choice = \"Accuracy\"  # Change this to \"AUROC\", \"Precision\", or \"CalibrationSlope\" as needed\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Loop through each unique Data_type and create separate plots\n",
    "for data_type, subset in df_avg.groupby(\"Data_type\"):\n",
    "    \n",
    "    \n",
    "    # Plot average Accuracy for each Method within Data_type\n",
    "    for method, subsubset in subset.groupby(\"Method\"):\n",
    "        if data_type == \"COVID Simulation\":\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            metric_choice = \"Accuracy\"\n",
    "            plt.plot(subsubset[\"Time\"], subsubset[metric_choice], label=f\"Avg {method}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Average \"+ metric_choice)\n",
    "            plt.title(f\"Average {metric_choice} Trends for {data_type}\")\n",
    "            plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        if data_type == \"Slow Change Simulation\":\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            metric_choice = \"AUPRC\"\n",
    "            plt.plot(subsubset[\"Time\"], subsubset[metric_choice], label=f\"Avg {method}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Average \"+ metric_choice)\n",
    "            plt.title(f\"Average {metric_choice} Trends for {data_type}\")\n",
    "            plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        if data_type == \"Outcome Prevalence Simulation\" or data_type == \"Multivariate Simulation\":\n",
    "            ### CALIBRATION SLOPE PLOT ###\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            metric_choice = \"CalibrationSlope\"\n",
    "            plt.plot(subsubset[\"Time\"], subsubset[metric_choice], label=f\"Avg {method}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Average \"+ metric_choice)\n",
    "            plt.title(f\"Average {metric_choice} Trends for {data_type}\")\n",
    "            plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "            ### O/E PLOT ###\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            metric_choice = \"OE\"\n",
    "            plt.plot(subsubset[\"Time\"], subsubset[metric_choice], label=f\"Avg {method}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Average \"+ metric_choice)\n",
    "            plt.title(f\"Average {metric_choice} Trends for {data_type}\")\n",
    "            plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "            ### CITL PLOT ###\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            metric_choice = \"CITL\"\n",
    "            plt.plot(subsubset[\"Time\"], subsubset[metric_choice], label=f\"Avg {method}\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Average \"+ metric_choice)\n",
    "            plt.title(f\"Average {metric_choice} Trends for {data_type}\")\n",
    "            plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot metrics and standard deviation bounds over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "metric_choice = \"Precision\"  # Change this to \"AUROC\", \"Accuracy\", or \"CalibrationSlope\" as needed\n",
    "\n",
    "for data_type in metrics_df[\"Data_type\"].unique():\n",
    "    \n",
    "    if data_type == \"COVID Simulation\":\n",
    "        metric_choice = \"Accuracy\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_subset = metrics_df[metrics_df[\"Data_type\"] == data_type]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=data_subset,\n",
    "            x=\"Time\",\n",
    "            y=metric_choice,\n",
    "            hue=\"Method\",\n",
    "            ci=\"sd\"  # or use \"se\" for standard error\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric_choice} Over Time with Standard Deviation: {data_type}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if data_type == \"Slow Change Simulation\":\n",
    "        metric_choice = \"AUPRC\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_subset = metrics_df[metrics_df[\"Data_type\"] == data_type]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=data_subset,\n",
    "            x=\"Time\",\n",
    "            y=metric_choice,\n",
    "            hue=\"Method\",\n",
    "            ci=\"sd\"  # or use \"se\" for standard error\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric_choice} Over Time with Standard Deviation: {data_type}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if data_type == \"Outcome Prevalence Simulation\" or data_type == \"Multivariate Simulation\":\n",
    "        metric_choice = \"CalibrationSlope\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_subset = metrics_df[metrics_df[\"Data_type\"] == data_type]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=data_subset,\n",
    "            x=\"Time\",\n",
    "            y=metric_choice,\n",
    "            hue=\"Method\",\n",
    "            ci=\"sd\"  # or use \"se\" for standard error\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric_choice} Over Time with Standard Deviation: {data_type}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        metric_choice = \"OE\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_subset = metrics_df[metrics_df[\"Data_type\"] == data_type]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=data_subset,\n",
    "            x=\"Time\",\n",
    "            y=metric_choice,\n",
    "            hue=\"Method\",\n",
    "            ci=\"sd\"  # or use \"se\" for standard error\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric_choice} Over Time with Standard Deviation: {data_type}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        metric_choice = \"CITL\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_subset = metrics_df[metrics_df[\"Data_type\"] == data_type]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=data_subset,\n",
    "            x=\"Time\",\n",
    "            y=metric_choice,\n",
    "            hue=\"Method\",\n",
    "            ci=\"sd\"  # or use \"se\" for standard error\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{metric_choice} Over Time with Standard Deviation: {data_type}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot barcharts for each method performance each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "metric_choice = \"CalibrationSlope\"  # Change this to \"AUROC\", \"Accuracy\", or \"Precision\" as needed\n",
    "\n",
    "# Loop through each unique Data_type to create separate box plots\n",
    "for data_type, subset in metrics_df.groupby(\"Data_type\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create box plot for Accuracy grouped by Time and Method\n",
    "    sns.boxplot(x=\"Time\", y=metric_choice, hue=\"Method\", data=subset, showfliers=False)\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(f\"{metric_choice} Distribution\")\n",
    "    plt.title(f\"{metric_choice} Distribution for {data_type}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of times each method was the \"best\" each month over all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_best_methods(metrics_df, metrics=[\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\", \"CITL\", \"OE\", \"AUPRC\"]):\n",
    "    \"\"\"\n",
    "    Finds the best method for each time point based on the given performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    metrics_df (DataFrame): The dataset containing performance metrics.\n",
    "    metrics (list): List of metric column names to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the count of times each method outperformed others for each metric.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric == \"OE\" or metric == \"CalbrationSlope\":\n",
    "            # For CalibrationSlope and O/E ratio, assume closer to 1 is better\n",
    "            # Find the method with the closest CalibrationSlope to 1 for each time point\n",
    "            closest_to_one = metrics_df.loc[metrics_df.groupby(\"Time\")[metric].apply(lambda x: (x - 1).abs().idxmin())][\"Method\"]\n",
    "            method_counts = closest_to_one.value_counts()\n",
    "        elif metric == \"CITL\":\n",
    "            # For CITL, assume closer to 0 is better\n",
    "            closest_to_zero = metrics_df.loc[metrics_df.groupby(\"Time\")[metric].apply(lambda x: x.abs().idxmin())][\"Method\"]\n",
    "            method_counts = closest_to_zero.value_counts()\n",
    "        else:\n",
    "            best_methods = metrics_df.loc[metrics_df.groupby(\"Time\")[metric].idxmax()][\"Method\"]\n",
    "            method_counts = best_methods.value_counts()\n",
    "        results[metric] = method_counts\n",
    "        print(f\"\\nNumber of times each method outperformed others ({metric}):\")\n",
    "        print(method_counts)\n",
    "\n",
    "    return results\n",
    "\n",
    "# count best methods for all simulations\n",
    "_ = count_best_methods(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall metrics for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ COVID Simulation Metrics ############\")\n",
    "covid_metrics_df = metrics_df[metrics_df[\"Data_type\"]==\"COVID Simulation\"]\n",
    "# Count best method for COVID simulation\n",
    "_ = count_best_methods(covid_metrics_df)\n",
    "# Compute min and max values for Accuracy, AUROC, and Precision for each method\n",
    "print(\"\\nMinimum and Maximum Metrics for COVID Simulation:\")\n",
    "min_max_metrics = covid_metrics_df.groupby(\"Method\")[[\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\", \"CITL\", \"OE\", \"AUPRC\"]].agg([\"min\", \"mean\", \"std\", \"max\"])\n",
    "print(min_max_metrics)\n",
    "\n",
    "print(\"############ Multivariate Simulation Metrics ############\")\n",
    "multivariate_metrics_df = metrics_df[metrics_df[\"Data_type\"]==\"Multivariate Simulation\"]\n",
    "_ = count_best_methods(multivariate_metrics_df)\n",
    "print(\"\\nMinimum and Maximum Metrics for Multivariate Simulation:\")\n",
    "min_max_metrics = multivariate_metrics_df.groupby(\"Method\")[[\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\", \"CITL\", \"OE\", \"AUPRC\"]].agg([\"min\", \"mean\", \"std\", \"max\"])\n",
    "print(min_max_metrics)\n",
    "\n",
    "print(\"############ Slow Change Simulation Metrics ############\")\n",
    "slow_metrics_df = metrics_df[metrics_df[\"Data_type\"]==\"Slow Change Simulation\"]\n",
    "_ = count_best_methods(slow_metrics_df)\n",
    "print(\"\\nMinimum and Maximum Metrics for Slow Change Simulation:\")\n",
    "min_max_metrics = slow_metrics_df.groupby(\"Method\")[[\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\", \"CITL\", \"OE\", \"AUPRC\"]].agg([\"min\", \"mean\", \"std\", \"max\"])\n",
    "print(min_max_metrics)\n",
    "\n",
    "print(\"############ Outcome Prevalence Simulation Metrics ############\")\n",
    "outcome_prev_metrics_df = metrics_df[metrics_df[\"Data_type\"]==\"Outcome Prevalence Simulation\"]\n",
    "_ = count_best_methods(outcome_prev_metrics_df)\n",
    "print(\"\\nMinimum and Maximum Metrics for Outcome Prevalence Simulation:\")\n",
    "min_max_metrics = outcome_prev_metrics_df.groupby(\"Method\")[[\"Accuracy\", \"AUROC\", \"Precision\", \"CalibrationSlope\", \"CITL\", \"OE\", \"AUPRC\"]].agg([\"min\", \"mean\", \"std\", \"max\"])\n",
    "print(min_max_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking methods based on mean accuracy across all data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average accuracy, AUROC, and precision per method\n",
    "method_avg_performance = metrics_df.groupby(\"Method\")[[\"Accuracy\", \"AUROC\", \"Precision\"]].mean()\n",
    "\n",
    "# Rank methods based on average accuracy\n",
    "method_avg_performance[\"Rank\"] = method_avg_performance[\"Accuracy\"].rank(method=\"dense\", ascending=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Method Rankings Based on Average Accuracy:\")\n",
    "print(method_avg_performance.sort_values(\"Rank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average rolling accuracy over time for all data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rolling averages for accuracy to smooth trends\n",
    "metrics_df[\"Rolling_Accuracy\"] = metrics_df.groupby(\"Method\")[\"Accuracy\"].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "# Plot accuracy trends over time for each method\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=metrics_df, x=\"Time\", y=\"Rolling_Accuracy\", hue=\"Method\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Rolling Average Accuracy\")\n",
    "plt.title(\"Accuracy Trends Over Time by Method\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1))\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
